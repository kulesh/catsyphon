# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

**Note**: This project uses [bd (beads)](https://github.com/steveyegge/beads) for issue tracking. Use `bd` commands instead of markdown TODOs. See AGENTS.md for workflow details.

## Project Overview

**CatSyphon** is a coding agent conversation log analysis and insights tool - a full-stack monorepo that parses, analyzes, and extracts insights from conversation logs generated by AI coding assistants (Claude Code, GitHub Copilot, Cursor, etc.).

**Current Status**: Phase 1 (Data Foundation) complete - Core parsing, ingestion, storage pipeline, and basic web UI implemented. Live directory watching and file deduplication operational.

## Architecture

### Monorepo Structure

```
catsyphon/
├── backend/              # Python FastAPI backend
│   ├── src/catsyphon/
│   │   ├── api/          # FastAPI REST endpoints
│   │   ├── parsers/      # Plugin-based log parsers (registry pattern)
│   │   ├── pipeline/     # ETL ingestion workflow
│   │   ├── db/           # SQLAlchemy ORM + repositories
│   │   ├── models/       # Data models (DB + Pydantic)
│   │   ├── tagging/      # AI metadata enrichment (OpenAI)
│   │   ├── cli.py        # Typer CLI commands
│   │   └── watch.py      # Live directory monitoring daemon
│   └── tests/            # Pytest test suite
│
├── frontend/             # React + TypeScript frontend
│   └── src/
│       ├── pages/        # Dashboard, ConversationList, ConversationDetail, Upload
│       ├── types/        # TypeScript API interfaces
│       └── lib/          # API client, TanStack Query setup
│
└── docs/                 # Technical documentation
```

### Technology Stack

**Backend:**
- Python 3.11+ with FastAPI (async REST API)
- PostgreSQL 15+ with SQLAlchemy 2.0 ORM
- Alembic for database migrations
- OpenAI gpt-4o-mini for AI-powered metadata tagging
- Typer CLI with Rich formatting
- Watchdog + python-daemon for live file monitoring
- uv for dependency management

**Frontend:**
- React 19 with TypeScript 5.9
- Vite 7 for build tooling
- Tailwind CSS 4 with shadcn/ui components
- TanStack React Query 5 for data fetching/caching with 15s auto-refresh polling
- React Router DOM 7
- pnpm for package management

**Code Quality:**
- Black (formatter), Ruff (linter), MyPy (strict type checking)
- Pytest + pytest-asyncio for testing

### Key Architectural Patterns

1. **Plugin-Based Parser System**: `ParserRegistry` dynamically discovers and routes log files to appropriate parsers (Claude Code implemented, extensible for other agents)

2. **Data Pipeline (ETL)**:
   ```
   Raw Logs → Parser → Deduplication → AI Tagging → PostgreSQL → REST API → Web UI
   ```

3. **Repository Pattern**: Data access abstracted through repository classes for type-safe queries

4. **Live Directory Watching**: Daemon monitors folders for new logs with automatic deduplication using content hashing

5. **Database Schema**: Highly normalized with Projects → Developers → Conversations → Epochs → Messages → Files Touched

## Development Commands

### Environment Setup

```bash
# Install tools (Python 3.11, Node 20, uv, pnpm)
mise install

# Start PostgreSQL
docker-compose up -d

# Backend setup
cd backend
uv sync --all-extras              # Install dependencies
uv run alembic upgrade head       # Run migrations

# Frontend setup
cd frontend
pnpm install                      # Install dependencies
```

### Backend Development

```bash
cd backend

# Run API server
uv run catsyphon serve                          # Production server on :8000
uv run uvicorn catsyphon.api.app:app --reload   # Dev server with hot reload

# CLI commands
uv run catsyphon ingest <path> --project "name"              # One-time log import
uv run catsyphon ingest <path> --enable-tagging              # Import with LLM tagging
uv run catsyphon watch <path> --project "name"               # Live directory watching
uv run catsyphon watch <path> --enable-tagging               # Watch with LLM tagging
uv run catsyphon watch <path> --verbose                      # With SQL query logs
uv run catsyphon db-status                                   # Database health check
uv run catsyphon version                                     # Show version

# Testing & quality
uv run pytest                                    # Run all tests
uv run pytest --cov=src/catsyphon               # With coverage
uv run pytest tests/test_api_conversations.py   # Single test file
uv run mypy src/                                # Type checking
uv run black src/ tests/                        # Format code
uv run ruff check src/ tests/                   # Lint

# Database migrations
uv run alembic revision --autogenerate -m "Description"
uv run alembic upgrade head
uv run alembic downgrade -1
```

### Frontend Development

```bash
cd frontend

pnpm dev                # Development server with HMR (Vite)
pnpm build              # Production build (tsc + vite)
pnpm lint               # ESLint
pnpm tsc --noEmit       # TypeScript type checking
pnpm preview            # Preview production build
```

### API Documentation

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/health
- **Readiness**: http://localhost:8000/ready

### Database Management

```bash
# Access PostgreSQL CLI
docker exec -it catsyphon-postgres psql -U catsyphon -d catsyphon

# View logs
docker-compose logs -f postgres

# Reset database (WARNING: deletes all data)
docker-compose down -v
docker-compose up -d
cd backend && uv run alembic upgrade head
```

## Important Patterns & Conventions

### Issue Tracking with bd (beads)

**CRITICAL**: Always use `bd` for task tracking, never markdown TODOs.

```bash
bd ready                                           # Show unblocked work
bd create "Title" -t bug|feature|task -p 0-4      # Create issue
bd update bd-42 --status in_progress              # Claim task
bd close bd-42 --reason "Completed"               # Complete task
```

**Always commit `.beads/issues.jsonl` with code changes** to keep issue state synced.

### Code Quality Requirements

- **Black**: Line length 88, auto-format before commits
- **Ruff**: Enforce E, F, I, N, W rules
- **MyPy**: Strict type checking required (`strict = true`)
- **Tests**: Maintain coverage, use pytest fixtures from `conftest.py`
- **Async**: Use async/await throughout backend (FastAPI + SQLAlchemy)

### Configuration Management

Environment variables in `.env` (copy from `.env.example`):

```bash
# Required
OPENAI_API_KEY=sk-xxx        # For AI tagging
POSTGRES_DB=catsyphon
POSTGRES_USER=catsyphon
POSTGRES_PASSWORD=catsyphon_dev_password

# Optional
ENVIRONMENT=development
LOG_LEVEL=INFO
API_PORT=8000
WATCH_POLL_INTERVAL=2

# Tagging (optional)
TAGGING_CACHE_DIR=.catsyphon_cache/tags  # Cache directory
TAGGING_CACHE_TTL_DAYS=30                # Cache expiration
TAGGING_ENABLE_CACHE=true                # Enable caching (reduces OpenAI costs)
```

Managed via Pydantic Settings in `backend/src/catsyphon/config.py`.

### Testing Guidelines

- **Unit Tests**: Parser logic, models, utilities
- **Integration Tests**: Database operations, repositories
- **API Tests**: REST endpoints with in-memory DB
- **Pipeline Tests**: Full ingestion workflow

Run specific test categories:
```bash
uv run pytest tests/test_parsers/           # Parser tests only
uv run pytest tests/test_api_*.py           # API endpoint tests
uv run pytest -k "deduplication"            # Tests matching pattern
```

## Key Endpoints

### REST API

**Conversations:**
- `GET /conversations` - List with pagination & filters
- `GET /conversations/{id}` - Single conversation detail
- `GET /conversations/{id}/messages` - Messages for conversation

**Statistics:**
- `GET /stats/overview` - Dashboard metrics
- `GET /stats/by-project` - Project-level stats
- `GET /stats/by-developer` - Developer patterns

**Metadata:**
- `GET /projects` - List projects
- `GET /developers` - List developers

**Upload:**
- `POST /upload` - Multipart file upload with progress

## Unique Features

1. **Live Directory Watching**: Daemon monitors folders for new log files with automatic deduplication via content hashing

2. **Real-Time Frontend Polling**: Dashboard and conversation list auto-refresh every 15 seconds with freshness indicators and new item highlighting

3. **AI-Powered Tagging**: OpenAI gpt-4o-mini enriches conversations with metadata (opt-in via `--enable-tagging` flag):
   - **Sentiment**: positive, neutral, negative, frustrated (with numeric score -1.0 to 1.0)
   - **Intent**: feature_add, bug_fix, refactor, learning, debugging, other
   - **Outcome**: success, partial, failed, abandoned, unknown
   - **Features/Problems**: Lists of capabilities discussed and blockers encountered
   - **Rule-Based Tags**: Automatic extraction of errors, tool usage, and patterns
   - **File-Based Cache**: 30-day TTL cache reduces costs by 80-90% on re-ingestion (~$10 per 1,000 conversations uncached)

4. **Deduplication System**: Hash-based duplicate detection prevents re-processing identical files

5. **Flexible Metadata**: JSONB columns for extensible, backward-compatible metadata storage

6. **Health Checks**: Startup validation for database connectivity, migration status, and readiness probes

## Data Flow Example

```
User uploads log → Upload API validates → Ingest Pipeline
  ↓
ParserRegistry.parse() → Normalize to ParsedConversation
  ↓
Deduplication.check() → Hash existing files
  ↓
AITagger.enrich() → Add sentiment/intent/outcome
  ↓
ConversationRepository.save() → Persist to PostgreSQL
  ↓
REST API → Frontend queries data → User views analytics
```

## Common Development Scenarios

### Adding a New Parser

1. Create `backend/src/catsyphon/parsers/new_agent.py`
2. Inherit from `BaseParser` with `parse()` method
3. Register in `parsers/registry.py`
4. Add tests in `tests/test_parsers/test_new_agent.py`
5. Update parser priority if needed

### Adding API Endpoints

1. Define Pydantic schema in `api/schemas.py`
2. Create route handler in `api/routes/`
3. Register router in `api/app.py`
4. Add tests in `tests/test_api_*.py`
5. Document endpoint in OpenAPI (automatic via FastAPI)

### Database Schema Changes

1. Update ORM models in `models/db.py`
2. Generate migration: `uv run alembic revision --autogenerate -m "Description"`
3. Review generated migration in `db/migrations/versions/`
4. Apply migration: `uv run alembic upgrade head`
5. Update repository methods if needed

### Running Tests Before Commits

```bash
cd backend
uv run pytest                      # All tests
uv run mypy src/                   # Type checking
uv run black src/ tests/           # Format
uv run ruff check src/ tests/      # Lint
```

## Git Workflow

1. Check for work: `bd ready`
2. Claim task: `bd update <id> --status in_progress`
3. Create branch: `git checkout -b feature/your-feature`
4. Make changes and test thoroughly
5. Commit: `git add . && git commit -m "Description"` (includes `.beads/issues.jsonl`)
6. Complete task: `bd close <id> --reason "Completed"`
7. Push and create PR

## Cost Estimates

Using OpenAI gpt-4o-mini for tagging:
- ~$10 per 1,000 conversations
- ~$10-15/month for a team of 10 developers

## Documentation

- [Implementation Plan](./docs/implementation-plan.md) - Comprehensive technical specifications
- [Agent Guidelines](./AGENTS.md) - Issue tracking workflow for AI agents
- [README.md](./README.md) - Project overview and quick start

## Notes

- Frontend uses shadcn/ui components (not Tremor) - check existing patterns in `frontend/src/pages/` before adding new components
- Backend uses async SQLAlchemy - always use `async with get_db()` pattern
- Watch daemon runs as background process - check PID file and logs in project root
- Parsers must return `ParsedConversation` objects - see `models/parsed.py` for structure
- Deduplication is hash-based on raw file content - do not modify `raw_logs` table directly
