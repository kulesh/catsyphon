# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

**Note**: This project uses [bd (beads)](https://github.com/steveyegge/beads) for issue tracking. Use `bd` commands instead of markdown TODOs. See AGENTS.md for workflow details.

## Project Overview

**CatSyphon** is a coding agent conversation log analysis and insights tool - a full-stack monorepo that parses, analyzes, and extracts insights from conversation logs generated by AI coding assistants (Claude Code, GitHub Copilot, Cursor, etc.).

**Current Status**: Phases 1-2 complete - Core parsing, ingestion, storage pipeline, and basic web UI implemented. Live directory watching, file deduplication, and **incremental parsing (10x-106x faster)** operational.

## Architecture

### Monorepo Structure

```
catsyphon/
├── backend/              # Python FastAPI backend
│   ├── src/catsyphon/
│   │   ├── api/          # FastAPI REST endpoints
│   │   ├── parsers/      # Plugin-based log parsers (registry pattern)
│   │   ├── pipeline/     # ETL ingestion workflow
│   │   ├── db/           # SQLAlchemy ORM + repositories
│   │   ├── models/       # Data models (DB + Pydantic)
│   │   ├── tagging/      # AI metadata enrichment (OpenAI)
│   │   ├── cli.py        # Typer CLI commands
│   │   └── watch.py      # Live directory monitoring daemon
│   └── tests/            # Pytest test suite
│
├── frontend/             # React + TypeScript frontend
│   └── src/
│       ├── pages/        # Dashboard, ConversationList, ConversationDetail, Upload
│       ├── types/        # TypeScript API interfaces
│       └── lib/          # API client, TanStack Query setup
│
└── docs/                 # Technical documentation
```

### Technology Stack

**Backend:**
- Python 3.11+ with FastAPI (async REST API)
- PostgreSQL 15+ with SQLAlchemy 2.0 ORM
- Alembic for database migrations
- OpenAI gpt-4o-mini for AI-powered metadata tagging
- Typer CLI with Rich formatting
- Watchdog + python-daemon for live file monitoring
- uv for dependency management

**Frontend:**
- React 19 with TypeScript 5.9
- Vite 7 for build tooling
- Tailwind CSS 4 with shadcn/ui components
- TanStack React Query 5 for data fetching/caching with 15s auto-refresh polling
- React Router DOM 7
- pnpm for package management

**Code Quality:**
- Black (formatter), Ruff (linter), MyPy (strict type checking)
- Pytest + pytest-asyncio for testing

### Key Architectural Patterns

1. **Plugin-Based Parser System**: `ParserRegistry` dynamically discovers and routes log files to appropriate parsers (Claude Code implemented, extensible for other agents)

2. **Data Pipeline (ETL)**:
   ```
   Raw Logs → Parser → Deduplication → AI Tagging → PostgreSQL → REST API → Web UI
   ```

3. **Repository Pattern**: Data access abstracted through repository classes for type-safe queries

4. **Live Directory Watching**: Daemon monitors folders for new logs with automatic deduplication using content hashing

5. **Database Schema**: Highly normalized with Projects → Developers → Conversations → Epochs → Messages → Files Touched

6. **Incremental Parsing**: Optimized parsing that only processes new content appended to log files
   - **Performance**: 10x to 106x faster than full reparse (depending on file size and append size)
   - **Memory**: 45x to 465x reduction in memory usage
   - **Change Detection**: Automatically detects file changes (APPEND, TRUNCATE, REWRITE, UNCHANGED)
   - **State Tracking**: Stores parsing state (offset, line number, hash) in `raw_logs` table
   - **Graceful Degradation**: Falls back to full reparse if incremental parsing fails

### Incremental Parsing Details

Incremental parsing dramatically improves performance for log files that are actively being appended to (e.g., during live Claude Code sessions or watch daemon monitoring).

**How It Works:**
1. First parse: Full parse creates RawLog entry with state (last_processed_offset, file_size_bytes, partial_hash)
2. Subsequent parses: Check for changes by comparing file size and partial hash
3. If APPEND detected: Parse only new content from last_processed_offset
4. If TRUNCATE/REWRITE detected: Full reparse required
5. If UNCHANGED: Skip processing entirely

**Performance Benchmarks:**
```
Small append (1 to 100 messages):     9.9x faster
Medium log (10 to 1000 messages):    36.6x faster
Large log (1 to 5000 messages):     106.0x faster
Multiple sequential appends:         14.0x faster

Memory reduction (1000 messages):     45x less memory
Memory reduction (50k messages):     465x less memory
```

**When Used:**
- Watch daemon processing existing conversations (auto-detected)
- CLI `ingest` command on previously processed files (auto-detected)
- Manual ingestion via API (auto-detected)

**Key Files:**
- `backend/src/catsyphon/parsers/incremental.py` - Change detection logic
- `backend/src/catsyphon/parsers/claude_code.py` - `parse_incremental()` method
- `backend/src/catsyphon/pipeline/ingestion.py` - Intelligent routing logic
- `backend/src/catsyphon/watch.py` - Watch daemon integration
- `backend/tests/test_performance.py` - Performance benchmarks

### Pipeline Metrics & Instrumentation

CatSyphon instruments every stage of the ingestion pipeline to measure latency and identify bottlenecks. Stage-level metrics are stored in the `ingestion_jobs` table and displayed in real-time on the Live Activity UI.

**Metrics Schema:**

Each ingestion job tracks the following metrics in a JSONB column:

```json
{
  "deduplication_check_ms": 45.2,     // File hash + DB lookup time
  "database_operations_ms": 234.8,    // Inserts, updates, queries
  "total_ms": 280.0                   // Sum of all stages
}
```

**How Metrics Work:**

1. **StageMetrics Helper** (`backend/src/catsyphon/pipeline/ingestion.py:40-64`):
   - Lightweight timing tracker that records start/end timestamps for each pipeline stage
   - Automatically calculates duration in milliseconds
   - Converts to dict for JSONB storage

2. **Ingestion Pipeline Instrumentation**:
   - `deduplication_check_ms`: Time to hash file and check for duplicates in database
   - `database_operations_ms`: Time for all SQLAlchemy operations (conversation insert, message bulk insert, file touched records)
   - `total_ms`: Sum of all stage timings

3. **Database Storage**:
   - Metrics stored in `ingestion_jobs.metrics` JSONB column
   - Allows flexible schema evolution without migrations
   - Efficient querying via PostgreSQL JSONB operators

4. **API Aggregation**:
   - `/api/ingestion/stats` returns aggregate metrics:
     - `avg_deduplication_check_ms`: Average dedup time across successful jobs
     - `avg_database_operations_ms`: Average DB time across successful jobs
     - `error_rates_by_stage`: Count of errors per stage (future)
   - Only includes successful jobs for meaningful averages
   - Failed jobs excluded to avoid skewing metrics

5. **Live Activity UI** (`frontend/src/pages/Ingestion.tsx`):
   - Real-time display of pipeline performance metrics
   - 3x2 grid of metric cards:
     - **Top Row**: Watch Directories, Total Jobs, Incremental Parsing
     - **Bottom Row**: Pipeline Performance, Error Breakdown, LLM Usage (placeholder)
   - Auto-refreshes every 10 seconds via TanStack Query polling
   - Individual job cards show stage-level breakdown when metrics available

**Interpreting Metrics:**

Common patterns and their meanings:

| Pattern | Meaning | Action |
|---------|---------|--------|
| High `deduplication_check_ms` (>100ms) | Large number of existing files in DB or slow file I/O | Consider indexing `raw_logs.content_hash` or optimizing file reading |
| High `database_operations_ms` (>500ms) | Large conversation with many messages or slow DB | Check database performance, consider batching optimizations |
| `total_ms` >> sum of stages | Unmeasured overhead or missing instrumentation | Add more stage tracking if needed |
| Empty metrics on duplicates | Duplicate detected before metrics initialized | Expected behavior for early-stage duplicates |

**Performance Impact:**

- Metrics tracking adds <5% overhead to ingestion
- StageMetrics uses <1KB memory per job
- Minimal impact on production performance
- Can be extended with more stages without code changes (just update StageMetrics calls)

**Extending Metrics:**

To add a new stage:

1. Add timing calls in `ingest_conversation()`:
   ```python
   metrics.start_stage("new_stage_name_ms")
   # ... stage logic ...
   metrics.end_stage("new_stage_name_ms")
   ```

2. Update documentation (this file)

3. Add frontend display in `Ingestion.tsx` (optional)

4. No database migration required (JSONB is schema-less)

**Testing:**

Comprehensive test coverage in `backend/tests/test_pipeline_metrics.py`:
- 9 unit tests for StageMetrics helper
- 6 integration tests for metrics population
- 7 API tests for endpoint responses
- 4 performance tests for overhead validation

**Key Files:**
- `backend/src/catsyphon/pipeline/ingestion.py` - StageMetrics class and instrumentation
- `backend/src/catsyphon/db/models.py:826-828` - metrics JSONB column
- `backend/src/catsyphon/db/repositories/ingestion_job.py:224-285` - get_stats() aggregation
- `backend/src/catsyphon/api/routes/ingestion.py:92-116` - /ingestion/stats endpoint
- `backend/src/catsyphon/api/schemas.py:387-390, 422-433` - Response schemas
- `frontend/src/pages/Ingestion.tsx` - Live Activity UI
- `backend/tests/test_pipeline_metrics.py` - Comprehensive test suite

## Development Commands

### Quick Start (Recommended)

Use the development script for easy management:

```bash
# Start everything (Colima, PostgreSQL, API server)
./scripts/dev.sh start

# Stop everything
./scripts/dev.sh stop

# Restart everything
./scripts/dev.sh restart

# Reset database (WARNING: deletes all data)
./scripts/dev.sh reset

# Check status
./scripts/dev.sh status
```

The script handles:
- Colima/Docker startup
- PostgreSQL port forwarding (required for Colima with vz driver)
- Database migrations
- File descriptor limits (increases from macOS default of 256 to 4096)
- API server with optimal worker count

### Manual Environment Setup

```bash
# Install tools (Python 3.11, Node 20, uv, pnpm)
mise install

# Start Colima (Docker runtime)
colima start

# Start PostgreSQL
docker-compose up -d

# Set up port forwarding (required for Colima with vz driver)
ssh -F ~/.config/colima/_lima/colima/ssh.config -L 5432:localhost:5432 -N -f lima-colima

# Backend setup
cd backend
uv sync --all-extras              # Install dependencies
uv run alembic upgrade head       # Run migrations

# Frontend setup
cd frontend
pnpm install                      # Install dependencies
```

### Known Issues

**Colima Port Forwarding**: Colima with `vz` (macOS Virtualization Framework) doesn't automatically forward Docker container ports to the host. The dev script handles this automatically, or manually run:
```bash
ssh -F ~/.config/colima/_lima/colima/ssh.config -L 5432:localhost:5432 -N -f lima-colima
```

**File Descriptor Limits**: macOS has a default soft limit of 256 file descriptors, which is too low for multi-worker setups. Increase with:
```bash
ulimit -n 4096
```

### Backend Development

```bash
cd backend

# Run API server
uv run catsyphon serve                          # Production server on :8000
uv run uvicorn catsyphon.api.app:app --reload   # Dev server with hot reload

# CLI commands (minimal - use Web UI for most features)
uv run catsyphon ingest <path> --project "name"              # One-time log import
uv run catsyphon ingest <path> --enable-tagging              # Import with LLM tagging
uv run catsyphon ingest <path> --force                       # Force re-ingest (skip dedup & replace existing)
# Note: --no-skip-duplicates is deprecated, use --force instead
# Note: For database status/stats, use Web UI Dashboard (http://localhost:8000)
# Note: For version info, use: pip show catsyphon

# Watch directories (via Web UI or API)
# Navigate to http://localhost:8000 → Ingestion → Watch Directories
# Click "Add Directory" to configure automatic monitoring
# Daemons start/stop via UI buttons or POST /watch/configs/{id}/start endpoint

# Testing & quality
python3 -m pytest                                # Run all tests (more stable than uv)
python3 -m pytest --cov=src/catsyphon           # With coverage
python3 -m pytest tests/test_api_conversations.py  # Single test file
python3 -m pytest tests/test_watch/             # Watch daemon tests
python3 -m mypy src/                            # Type checking
python3 -m black src/ tests/                    # Format code
python3 -m ruff check src/ tests/               # Lint
python3 -m ruff check --fix src/ tests/         # Lint with auto-fix

# Database migrations
uv run alembic revision --autogenerate -m "Description"
uv run alembic upgrade head
uv run alembic downgrade -1
```

### Frontend Development

```bash
cd frontend

pnpm dev                # Development server with HMR (Vite)
pnpm build              # Production build (tsc + vite)
pnpm test               # Run Vitest tests in watch mode
pnpm test -- --run      # Run tests once (CI mode)
pnpm run test:coverage  # Run tests with coverage report
pnpm run test:ui        # Interactive test UI
pnpm lint               # ESLint
pnpm tsc --noEmit       # TypeScript type checking
pnpm preview            # Preview production build
```

### API Documentation

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/health
- **Readiness**: http://localhost:8000/ready

### Database Management

```bash
# Access PostgreSQL CLI
docker exec -it catsyphon-postgres psql -U catsyphon -d catsyphon

# View logs
docker-compose logs -f postgres

# Reset database (WARNING: deletes all data)
docker-compose down -v
docker-compose up -d
cd backend && uv run alembic upgrade head
```

## Important Patterns & Conventions

### Issue Tracking with bd (beads)

**CRITICAL**: Always use `bd` for task tracking, never markdown TODOs.

```bash
bd ready                                           # Show unblocked work
bd create "Title" -t bug|feature|task -p 0-4      # Create issue
bd update bd-42 --status in_progress              # Claim task
bd close bd-42 --reason "Completed"               # Complete task
```

**Always commit `.beads/issues.jsonl` with code changes** to keep issue state synced.

### Code Quality Requirements

- **Black**: Line length 88, auto-format before commits
- **Ruff**: Enforce E, F, I, N, W rules
- **MyPy**: Strict type checking required (`strict = true`)
- **Tests**: Maintain coverage, use pytest fixtures from `conftest.py`
- **Async**: Use async/await throughout backend (FastAPI + SQLAlchemy)

### Configuration Management

Environment variables in `.env` (copy from `.env.example`):

```bash
# Required
OPENAI_API_KEY=sk-xxx        # For AI tagging
POSTGRES_DB=catsyphon
POSTGRES_USER=catsyphon
POSTGRES_PASSWORD=catsyphon_dev_password

# Optional
ENVIRONMENT=development
API_PORT=8000
WATCH_POLL_INTERVAL=2

# Logging Configuration
LOG_LEVEL=INFO                              # DEBUG, INFO, WARNING, ERROR, CRITICAL
# LOG_DIR=/custom/log/path                  # Leave empty for XDG default (~/.local/state/catsyphon/logs)
# LOG_FORMAT=standard                       # standard or json (for production/log aggregation)
# LOG_CONSOLE_ENABLED=true                  # Enable console output
# LOG_FILE_ENABLED=true                     # Enable file-based logging
# LOG_MAX_BYTES=10485760                    # 10MB max per log file
# LOG_BACKUP_COUNT=5                        # Keep 5 backup files when rotating
# LOG_TO_STDOUT=true                        # Log INFO/DEBUG to stdout
# LOG_TO_STDERR=true                        # Log WARNING/ERROR/CRITICAL to stderr

# LLM Interaction Logging (for debugging and cost tracking)
# LLM_LOGGING_ENABLED=false                 # Enable detailed OpenAI API logging
# LLM_LOG_REQUESTS=true                     # Log API requests
# LLM_LOG_RESPONSES=true                    # Log API responses
# LLM_LOG_TOKENS=true                       # Log token usage

# Tagging (optional)
# Cache directory follows XDG Base Directory spec:
#   - $XDG_CACHE_HOME/catsyphon/tags (if XDG_CACHE_HOME set)
#   - $HOME/.cache/catsyphon/tags (default)
#   - .catsyphon_cache/tags (fallback for dev/testing)
TAGGING_CACHE_DIR=~/.cache/catsyphon/tags  # XDG-compliant cache directory
TAGGING_CACHE_TTL_DAYS=30                   # Cache expiration
TAGGING_ENABLE_CACHE=true                   # Enable caching (reduces OpenAI costs)
```

Managed via Pydantic Settings in `backend/src/catsyphon/config.py`.

### Logging System

CatSyphon uses a centralized logging system with separate log files and configurable output:

**Log File Structure** (XDG-compliant: `~/.local/state/catsyphon/logs`):
```
logs/
├── application.log           # INFO and DEBUG messages
├── error.log                 # WARNING, ERROR, CRITICAL messages
├── api-application.log       # API server logs
├── api-error.log            # API errors
├── watch-{id}-application.log  # Per-daemon logs
├── watch-{id}-error.log
└── llm/
    └── requests.log          # OpenAI API interactions (when enabled)
```

**Features**:
- Automatic log rotation (10MB max, 5 backups)
- Separate stdout (INFO/DEBUG) and stderr (WARNING+) streams
- Context-specific log files for watch daemons
- Optional LLM interaction logging for debugging and cost tracking
- XDG Base Directory compliant

**LLM Logging**:
When `LLM_LOGGING_ENABLED=true`, all OpenAI API interactions are logged to `llm/requests.log` with:
- Request details (model, prompt preview, parameters)
- Response details (content preview, token usage, timing)
- Error information for failed requests
- Cache hits (no API call made)

Example LLM log entry:
```
[2025-11-18 13:45:23] [INFO] REQUEST: {"type": "request", "request_id": "test-123_1700318723000", "model": "gpt-4o-mini", "session_id": "test-123", "message_count": 5, "parameters": {"max_tokens": 2000, "temperature": 0.3}, "prompt_length": 1234}
[2025-11-18 13:45:24] [INFO] RESPONSE: {"type": "response", "request_id": "test-123_1700318723000", "model": "gpt-4o-mini", "finish_reason": "stop", "duration_ms": 1250.5, "tokens": {"prompt": 156, "completion": 89, "total": 245}}
```

### Testing Guidelines

- **Unit Tests**: Parser logic, models, utilities
- **Integration Tests**: Database operations, repositories
- **API Tests**: REST endpoints with in-memory DB
- **Pipeline Tests**: Full ingestion workflow

Run specific test categories:
```bash
uv run pytest tests/test_parsers/           # Parser tests only
uv run pytest tests/test_api_*.py           # API endpoint tests
uv run pytest -k "deduplication"            # Tests matching pattern
```

## Key Endpoints

### REST API

**Conversations:**
- `GET /conversations` - List with pagination & filters
- `GET /conversations/{id}` - Single conversation detail
- `GET /conversations/{id}/messages` - Messages for conversation

**Statistics:**
- `GET /stats/overview` - Dashboard metrics
- `GET /stats/by-project` - Project-level stats
- `GET /stats/by-developer` - Developer patterns

**Metadata:**
- `GET /projects` - List projects
- `GET /developers` - List developers

**Upload:**
- `POST /upload` - Multipart file upload with progress

## Unique Features

1. **Live Directory Watching**: Daemon monitors folders for new log files with automatic deduplication via content hashing

2. **Real-Time Frontend Polling**: Dashboard and conversation list auto-refresh every 15 seconds with freshness indicators and new item highlighting

3. **AI-Powered Tagging**: OpenAI gpt-4o-mini enriches conversations with metadata (opt-in via `--enable-tagging` flag):
   - **Sentiment**: positive, neutral, negative, frustrated (with numeric score -1.0 to 1.0)
   - **Intent**: feature_add, bug_fix, refactor, learning, debugging, other
   - **Outcome**: success, partial, failed, abandoned, unknown
   - **Features/Problems**: Lists of capabilities discussed and blockers encountered
   - **Rule-Based Tags**: Automatic extraction of errors, tool usage, and patterns
   - **File-Based Cache**: 30-day TTL cache reduces costs by 80-90% on re-ingestion (~$10 per 1,000 conversations uncached)

4. **Deduplication System**: Hash-based duplicate detection prevents re-processing identical files

5. **Flexible Metadata**: JSONB columns for extensible, backward-compatible metadata storage

6. **Health Checks**: Startup validation for database connectivity, migration status, and readiness probes

## Data Flow Example

```
User uploads log → Upload API validates → Ingest Pipeline
  ↓
ParserRegistry.parse() → Normalize to ParsedConversation
  ↓
Deduplication.check() → Hash existing files
  ↓
AITagger.enrich() → Add sentiment/intent/outcome
  ↓
ConversationRepository.save() → Persist to PostgreSQL
  ↓
REST API → Frontend queries data → User views analytics
```

## Common Development Scenarios

### Adding a New Parser

1. Create `backend/src/catsyphon/parsers/new_agent.py`
2. Inherit from `BaseParser` with `parse()` method
3. Register in `parsers/registry.py`
4. Add tests in `tests/test_parsers/test_new_agent.py`
5. Update parser priority if needed

### Adding API Endpoints

1. Define Pydantic schema in `api/schemas.py`
2. Create route handler in `api/routes/`
3. Register router in `api/app.py`
4. Add tests in `tests/test_api_*.py`
5. Document endpoint in OpenAPI (automatic via FastAPI)

### Database Schema Changes

1. Update ORM models in `models/db.py`
2. Generate migration: `uv run alembic revision --autogenerate -m "Description"`
3. Review generated migration in `db/migrations/versions/`
4. Apply migration: `uv run alembic upgrade head`
5. Update repository methods if needed

### Running Tests Before Commits

```bash
cd backend
uv run pytest                      # All tests
uv run mypy src/                   # Type checking
uv run black src/ tests/           # Format
uv run ruff check src/ tests/      # Lint
```

## Git Workflow

1. Check for work: `bd ready`
2. Claim task: `bd update <id> --status in_progress`
3. Create branch: `git checkout -b feature/your-feature`
4. Make changes and test thoroughly
5. Commit: `git add . && git commit -m "Description"` (includes `.beads/issues.jsonl`)
6. Complete task: `bd close <id> --reason "Completed"`
7. Push and create PR

## Cost Estimates

Using OpenAI gpt-4o-mini for tagging:
- ~$10 per 1,000 conversations
- ~$10-15/month for a team of 10 developers

## Documentation

- [Implementation Plan](./docs/implementation-plan.md) - Comprehensive technical specifications
- [Agent Guidelines](./AGENTS.md) - Issue tracking workflow for AI agents
- [README.md](./README.md) - Project overview and quick start

## Notes

- Frontend uses shadcn/ui components (not Tremor) - check existing patterns in `frontend/src/pages/` before adding new components
- Backend uses async SQLAlchemy - always use `async with get_db()` pattern
- Watch daemon runs as background process - check PID file and logs in project root
- Parsers must return `ParsedConversation` objects - see `models/parsed.py` for structure
- Deduplication is hash-based on raw file content - do not modify `raw_logs` table directly
